{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb0ab27-b3f2-458f-bff7-0b2bab9e9b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Using cached keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, markdown, grpcio, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 grpcio-1.70.0 keras-3.8.0 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493d7420-b7a3-4faf-820d-58898c0b07aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
      "  Using cached sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Using cached sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.13.0 imblearn-0.0 sklearn-compat-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b17503-cf96-4d01-8441-f675b1b5e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[42 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-pip-egg-info-6syqay84/lime.egg-info\n",
      "  \u001b[31m   \u001b[0m writing /tmp/pip-pip-egg-info-6syqay84/lime.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to /tmp/pip-pip-egg-info-6syqay84/lime.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to /tmp/pip-pip-egg-info-6syqay84/lime.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to /tmp/pip-pip-egg-info-6syqay84/lime.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m writing manifest file '/tmp/pip-pip-egg-info-6syqay84/lime.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest file '/tmp/pip-pip-egg-info-6syqay84/lime.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-fj4men74/lime_c8da1bf0de614a8bbc516f1c794ae6c6/setup.py\", line 3, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(name='lime',\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 983, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/dist.py\", line 999, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 312, in run\n",
      "  \u001b[31m   \u001b[0m     self.find_sources()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 320, in find_sources\n",
      "  \u001b[31m   \u001b[0m     mm.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 548, in run\n",
      "  \u001b[31m   \u001b[0m     self.prune_file_list()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/sdist.py\", line 162, in prune_file_list\n",
      "  \u001b[31m   \u001b[0m     super().prune_file_list()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/command/sdist.py\", line 380, in prune_file_list\n",
      "  \u001b[31m   \u001b[0m     base_dir = self.distribution.get_fullname()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 272, in get_fullname\n",
      "  \u001b[31m   \u001b[0m     return _distribution_fullname(self.get_name(), self.get_version())\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 290, in _distribution_fullname\n",
      "  \u001b[31m   \u001b[0m     canonicalize_version(version, strip_trailing_zero=False),\n",
      "  \u001b[31m   \u001b[0m TypeError: canonicalize_version() got an unexpected keyword argument 'strip_trailing_zero'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3b7d48-3c20-43c8-b225-222b6e69f899",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlime_text\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, TFBertModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import shap\n",
    "import lime.lime_text\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "print(\" All required libraries installed succesfully!!!!...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e9d16c-2805-4582-8cbf-2333eba61ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "labeled_data = pd.read_csv('labeled_data.csv')\n",
    "emoji_sentiment = pd.read_excel('emoji_sentiment.xlsx')\n",
    "print(\"All datasets loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ada1409-ba16-49b7-950f-d8d7fb6d1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148f7add-b6da-4755-a2e8-2ca0b30960c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data['clean_text'] = labeled_data['tweet'].apply(clean_text)  # Fixed column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3c88a7d-4069-4a10-ae90-061bde34d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "labeled_data['label'] = label_encoder.fit_transform(labeled_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffbbbd5d-0168-46d9-80fb-e9969fa08fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(labeled_data['clean_text'], labeled_data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1675346-44bf-4a63-9fde-86ae2de19794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15ce377-f599-4bd5-bf0b-5626ab30e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings (Word2Vec)\n",
    "tokenized_texts = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e613530f-3be7-45a5-9b7b-21561d56b4e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\haindavi\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# BERT Embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63592adb-2842-4b3e-b5ae-bef29f833ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts):\n",
    "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = bert_model(tokens['input_ids'])\n",
    "    return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fdaea1f-9095-46c5-ae1d-4a9a67eebe85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'LayerNorm' (type LayerNormalization).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[19826,53,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Mul] name: \n\nCall arguments received by layer 'LayerNorm' (type LayerNormalization):\n  • inputs=tf.Tensor(shape=(19826, 53, 768), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_bert \u001b[38;5;241m=\u001b[39m get_bert_embeddings(X_train\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m, in \u001b[0;36mget_bert_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bert_embeddings\u001b[39m(texts):\n\u001b[0;32m      2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(texts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m bert_model(tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1209\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1210\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1211\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1212\u001b[0m         token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1213\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1214\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1215\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1216\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1217\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1218\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1219\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1220\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1221\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1222\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1223\u001b[0m         training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m   1224\u001b[0m     )\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:887\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    885\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill(dims\u001b[38;5;241m=\u001b[39minput_shape, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 887\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    888\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    889\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    890\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m    891\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    892\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    893\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m    894\u001b[0m )\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m attention_mask_shape \u001b[38;5;241m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:197\u001b[0m, in \u001b[0;36mTFBertEmbeddings.call\u001b[1;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[0;32m    195\u001b[0m token_type_embeds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings, indices\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[0;32m    196\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds \u001b[38;5;241m+\u001b[39m token_type_embeds\n\u001b[1;32m--> 197\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(inputs\u001b[38;5;241m=\u001b[39mfinal_embeddings)\n\u001b[0;32m    198\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(inputs\u001b[38;5;241m=\u001b[39mfinal_embeddings, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_embeddings\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'LayerNorm' (type LayerNormalization).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[19826,53,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Mul] name: \n\nCall arguments received by layer 'LayerNorm' (type LayerNormalization):\n  • inputs=tf.Tensor(shape=(19826, 53, 768), dtype=float32)"
     ]
    }
   ],
   "source": [
    "X_train_bert = get_bert_embeddings(X_train.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507684e7-2ec7-4d8a-aef9-765dc02d2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bert = get_bert_embeddings(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ca308-2e38-4d2a-9698-172589d57d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing Data with SMOTE and Undersampling\n",
    "smote = SMOTE(random_state=42)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_tfidf, y_train)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_bal, y_train_bal)\n",
    "print(\"Balanced dataset shape:\", Counter(y_train_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae033b7-65ba-4dc8-8d77-552378de7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Development (Hybrid CNN-RNN)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=X_train_tfidf.shape[1]),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74dfbf-da6e-4f2f-803b-950adecbfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1591b44-ff29-453c-b016-4c49c513aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model.fit(X_train_bal.toarray(), y_train_bal, epochs=10, batch_size=32, validation_data=(X_test_tfidf.toarray(), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bdeaf8-89ab-45d4-9942-e7d7ce2e3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "eval_results = model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "print(f\"Test Accuracy: {eval_results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678f678-64b9-4c3d-94f7-e75d02ca7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability (SHAP & LIME)\n",
    "explainer = shap.Explainer(model, X_test_tfidf.toarray())\n",
    "shap_values = explainer(X_test_tfidf.toarray())\n",
    "shap.summary_plot(shap_values, X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3e0de-ebff-4c28-8905-b9a22a250b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_text.LimeTextExplainer(class_names=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116e52b-e832-4d05-a9fe-45ef57a9daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(text_instance):\n",
    "    explanation = lime_explainer.explain_instance(text_instance, model.predict)\n",
    "    explanation.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2e0a8-d372-4087-97f9-835bbb334dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model successfully trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f26d2-ff71-401b-ab92-19039059a72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77941525-e9cd-46ed-83ee-bcae836587bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
      "       'class', 'tweet'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(labeled_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37b973-6d49-4bc2-b60a-7cec91d43f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
