{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3b7d48-3c20-43c8-b225-222b6e69f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All required libraries installed succesfully!!!!...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import shap\n",
    "import lime.lime_text\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "print(\" All required libraries installed succesfully!!!!...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e9d16c-2805-4582-8cbf-2333eba61ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "labeled_data = pd.read_csv('labeled_data.csv')\n",
    "emoji_sentiment = pd.read_excel('emoji_sentiment.xlsx')\n",
    "print(\"All datasets loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ada1409-ba16-49b7-950f-d8d7fb6d1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148f7add-b6da-4755-a2e8-2ca0b30960c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data['clean_text'] = labeled_data['tweet'].apply(clean_text)  # Fixed column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3c88a7d-4069-4a10-ae90-061bde34d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "labeled_data['label'] = label_encoder.fit_transform(labeled_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffbbbd5d-0168-46d9-80fb-e9969fa08fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(labeled_data['clean_text'], labeled_data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1675346-44bf-4a63-9fde-86ae2de19794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15ce377-f599-4bd5-bf0b-5626ab30e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings (Word2Vec)\n",
    "tokenized_texts = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e613530f-3be7-45a5-9b7b-21561d56b4e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\haindavi\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# BERT Embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63592adb-2842-4b3e-b5ae-bef29f833ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts):\n",
    "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = bert_model(tokens['input_ids'])\n",
    "    return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fdaea1f-9095-46c5-ae1d-4a9a67eebe85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'LayerNorm' (type LayerNormalization).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[19826,53,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Mul] name: \n\nCall arguments received by layer 'LayerNorm' (type LayerNormalization):\n  • inputs=tf.Tensor(shape=(19826, 53, 768), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_bert \u001b[38;5;241m=\u001b[39m get_bert_embeddings(X_train\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m, in \u001b[0;36mget_bert_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bert_embeddings\u001b[39m(texts):\n\u001b[0;32m      2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(texts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m bert_model(tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1209\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1210\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1211\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1212\u001b[0m         token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1213\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1214\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1215\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1216\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1217\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1218\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1219\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1220\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1221\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1222\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1223\u001b[0m         training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m   1224\u001b[0m     )\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:887\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    885\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill(dims\u001b[38;5;241m=\u001b[39minput_shape, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 887\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    888\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    889\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    890\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m    891\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    892\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    893\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m    894\u001b[0m )\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m attention_mask_shape \u001b[38;5;241m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[1;32mE:\\haindavi\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:197\u001b[0m, in \u001b[0;36mTFBertEmbeddings.call\u001b[1;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[0;32m    195\u001b[0m token_type_embeds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings, indices\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[0;32m    196\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds \u001b[38;5;241m+\u001b[39m token_type_embeds\n\u001b[1;32m--> 197\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(inputs\u001b[38;5;241m=\u001b[39mfinal_embeddings)\n\u001b[0;32m    198\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(inputs\u001b[38;5;241m=\u001b[39mfinal_embeddings, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_embeddings\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'LayerNorm' (type LayerNormalization).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[19826,53,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Mul] name: \n\nCall arguments received by layer 'LayerNorm' (type LayerNormalization):\n  • inputs=tf.Tensor(shape=(19826, 53, 768), dtype=float32)"
     ]
    }
   ],
   "source": [
    "X_train_bert = get_bert_embeddings(X_train.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507684e7-2ec7-4d8a-aef9-765dc02d2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bert = get_bert_embeddings(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ca308-2e38-4d2a-9698-172589d57d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing Data with SMOTE and Undersampling\n",
    "smote = SMOTE(random_state=42)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_tfidf, y_train)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_bal, y_train_bal)\n",
    "print(\"Balanced dataset shape:\", Counter(y_train_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae033b7-65ba-4dc8-8d77-552378de7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Development (Hybrid CNN-RNN)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=X_train_tfidf.shape[1]),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74dfbf-da6e-4f2f-803b-950adecbfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1591b44-ff29-453c-b016-4c49c513aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model.fit(X_train_bal.toarray(), y_train_bal, epochs=10, batch_size=32, validation_data=(X_test_tfidf.toarray(), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bdeaf8-89ab-45d4-9942-e7d7ce2e3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "eval_results = model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "print(f\"Test Accuracy: {eval_results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678f678-64b9-4c3d-94f7-e75d02ca7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability (SHAP & LIME)\n",
    "explainer = shap.Explainer(model, X_test_tfidf.toarray())\n",
    "shap_values = explainer(X_test_tfidf.toarray())\n",
    "shap.summary_plot(shap_values, X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3e0de-ebff-4c28-8905-b9a22a250b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_text.LimeTextExplainer(class_names=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116e52b-e832-4d05-a9fe-45ef57a9daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(text_instance):\n",
    "    explanation = lime_explainer.explain_instance(text_instance, model.predict)\n",
    "    explanation.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2e0a8-d372-4087-97f9-835bbb334dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model successfully trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f26d2-ff71-401b-ab92-19039059a72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77941525-e9cd-46ed-83ee-bcae836587bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
      "       'class', 'tweet'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(labeled_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37b973-6d49-4bc2-b60a-7cec91d43f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
